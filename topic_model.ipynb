{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topic-model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEsZzlG6oZfEHcvpXO8nwE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fromakim/2021Election_Analysis/blob/main/topic_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Dev Environment"
      ],
      "metadata": {
        "id": "tp4pRUfX_sOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tomotopy\n",
        "!pip install hangul_utils"
      ],
      "metadata": {
        "id": "c0IOURC3AJjF",
        "outputId": "df833c39-e9fe-4345-d651-26bc61d39af6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tomotopy in /usr/local/lib/python3.7/dist-packages (0.12.2)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tomotopy) (1.19.5)\n",
            "Requirement already satisfied: hangul_utils in /usr/local/lib/python3.7/dist-packages (0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
      ],
      "metadata": {
        "id": "nt-d7j6CVZCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kakao/khaiii.git\n",
        "!pip install cmake\n",
        "!mkdir build\n",
        "!cd build && cmake /content/khaiii\n",
        "!cd /content/build/ && make all\n",
        "!cd /content/build/ && make resource\n",
        "!cd /content/build && make install\n",
        "!cd /content/build && make package_python\n",
        "!pip install /content/build/package_python"
      ],
      "metadata": {
        "id": "iCzuF94pVdw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from pykospacing import Spacing\n",
        "from khaiii import KhaiiiApi\n",
        "from hangul_utils import split_syllables, join_jamos"
      ],
      "metadata": {
        "id": "AJM7mXEuU9xk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tomotopy as tp"
      ],
      "metadata": {
        "id": "q22RDph9AGZt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "VSn6jZqK_rUw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "zZDJvi7n_uyF",
        "outputId": "82442c72-1170-457b-96a5-3398e745eec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Import Data"
      ],
      "metadata": {
        "id": "JX1LRGHnAh-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/election_sample/tweet.csv')"
      ],
      "metadata": {
        "id": "XNcvuPl6ABH5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['text']"
      ],
      "metadata": {
        "id": "68ShVtlDAmUt",
        "outputId": "600e4dcd-7c57-417b-f09e-d4f58fbe76c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       ìœ¤ì„ì—´ì´ëž‘ ì´ìž¬ëª…ì´ëž‘ ì‚¬ëž‘ì˜ ë„í”¼í•´ì„œ ì‹¬ìƒì •ì´ ëŒ€í†µë ¹ëœ ë‹¤ìŒì— ë™ì„±í˜¼ ë²•ì œí™” ì‹œì¼œì„œ...\n",
              "1       [ì†ë³´] 20ëŒ€ ëŒ€í†µë ¹ ì„ ê±° í›„ë³´ í™•ì •\\në”ë¶ˆì–´ë¯¼ì£¼ë‹¹ ì´ìž¬ëª…\\nêµ­ë¯¼ì˜íž˜ ìœ¤ì„ì—´\\nì •...\n",
              "2       ì •í™•ížˆ í‘œí˜„í•´ì•¼ í•©ë‹ˆë‹¤.\\n\\n- ì´ìž¬ëª…ì„ ì•ˆì°ëŠ”ê±´ ë‹¹ì—°.\\n- ê¸°ê¶Œì„ í•˜ë©´ ì´ìž¬ëª…...\n",
              "3       ì´ ì‹¬ìƒì •ì˜ 1ë¶„ì„ ì–´ë–»ê²Œ ìžŠì„ ìˆ˜ ìžˆì„ê¹Œ. ë‚˜ëŠ” ì ˆëŒ€ ëª» ìžŠëŠ”ë‹¤. https://...\n",
              "4       ì‹¬ìƒì •ì„ ë½‘ëŠ” í‘œê°€ ì‚¬í‘œê°€ ì•„ë‹ˆëž€ê±¸ ìš°ë¦¬ ì—¬ì„±ë“¤ì´ ë³´ì—¬ì¤˜ì•¼ í•œë‹¤. ì¸êµ¬ ì ˆë°˜ì˜ ì—¬ì„±...\n",
              "                              ...                        \n",
              "5316    ìœ¤ì„ì—´ì€ ëŠìž„ì—†ì´ ê¹Œë©´ì„œ\\nì´ìž¬ëª…ì— ëŒ€í•´ ì¼ì–¸ë°˜êµ¬ë„ ì—†ëŠ”\\në‹¹ì‹ ë„ ë˜‘ê°™ì€ ì—­ì‚¬ì˜ ì£„...\n",
              "5317    ðŸ’šë‹¤ìŒì£¼ í† ìš”ì¼ ì„œì´ˆì§‘íšŒ ì•„ì  ë‹¤ðŸ’š\\nðŸ“ŒëŒ€ìž¥ë™ íŠ¹ê²€\\nðŸ“Œí•©ìˆ˜ë¶€ ì„¤ì¹˜\\nðŸ“Œì´ìž¬ëª… êµ¬ì†\\...\n",
              "5318    ì•„ë‹ˆ ì• ì´ˆì— ì´ìž¬ëª…ì´ ë¯¼ì£¼ë‹¹ì•ˆë°–ì—ì„œ ë°˜ë¬¸ì§ˆë¡œ ì§€ê¸ˆê¹Œì§€ ì»¤ì˜¨ê±° ì‚¬ì‹¤ì¸ë° ë¬´ìŠ¨ ë¬¸ìž¬ì¸ì„...\n",
              "5319    ì—¬ê¸°ëŠ” ê²½ë¶ìš¸ì§„ ì£¼ë³€ì— ë§Žì€ ì‚¬ëžŒë“¤ì´ ê³µí†µì ìœ¼ë¡œ í•˜ëŠ”ë§ â€œìœ¤ì„ë ¬ ë„ˆë¬´ ìˆ˜ì¤€ë–¨ì–´ì§„ë‹¤â€...\n",
              "5320    ê¹€ì¢…í˜ ì „ êµ­ìž¥ \"ì •ì§„ìƒì´ ë­”ê°€ ê³ ë¦¬ê°€ ë¼ ìžˆê±°ë‚˜ ì˜ì‹¬ë°›ê³  ìžˆëŠ”ë° ë–¡ í•˜ë‹ˆ ë¶€ì‹¤ìž¥ìœ¼...\n",
              "Name: text, Length: 5321, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "oKEUWv3oP8Mb",
        "outputId": "0819b6e1-82d6-4c9a-8bdc-4556253eca14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/stopwords.csv')"
      ],
      "metadata": {
        "id": "zK-qUcoyen4L"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Preprocessing"
      ],
      "metadata": {
        "id": "hgiYglJAb_rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacing = Spacing()\n",
        "api = KhaiiiApi()"
      ],
      "metadata": {
        "id": "NdOWmQv6cNdR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relations = ['JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ', 'JX', 'JC']           # ê´€ê³„ì–¸ (ì¡°ì‚¬)\n",
        "dependents = ['EP', 'EF', 'EC', 'ETN', 'ETM', 'XPN', 'XSN', 'XSV', 'XSA', 'XR']     # ì˜ì¡´ê²© (ì–´ë¯¸, ì ‘ì‚¬)\n",
        "symbols = ['SF', 'SP', 'SS', 'SE', 'SO', 'SL', 'SH', 'SW', 'SWK', 'SN']             # ê¸°í˜¸"
      ],
      "metadata": {
        "id": "qX4jT6-RcUCq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unused = []\n",
        "unused.extend(relations)\n",
        "unused.extend(dependents)\n",
        "unused.extend(symbols)"
      ],
      "metadata": {
        "id": "-rSYzl4Ecm2Z"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(list_of_lists):\n",
        "    '''\n",
        "    Return flattened list from list of lists.\n",
        "    '''\n",
        "    return [y for x in list_of_lists for y in x]"
      ],
      "metadata": {
        "id": "5Rx8F_w7caZi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def khaiii_analyze(phrase):\n",
        "    '''\n",
        "    Return Lexicons analyzed by Khaiii Analyzer.\n",
        "    '''\n",
        "    return [z.lex for z in flatten([y.morphs for y in api.analyze(phrase)]) if z.tag not in unused]"
      ],
      "metadata": {
        "id": "EHBDdcfzca6e"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['original'] = tweets['text']"
      ],
      "metadata": {
        "id": "o_VHV40PchuS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Erase Non-Korean and Non-English Keywords\n",
        "tweets['text'] = tweets['text'].str.replace('[^A-Za-zê°€-íž£0-9ã„±-ã…Žã…-ã…£.,/ ]', ' ')"
      ],
      "metadata": {
        "id": "gwVj6Zbgbm8B"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['text'] = tweets['text'].map(split_syllables)"
      ],
      "metadata": {
        "id": "yYwUEjMdcIaZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['text'] = tweets['text'].map(join_jamos)"
      ],
      "metadata": {
        "id": "8x9ynFcgcZBN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['text'] = tweets['text'].map(lambda x : x.replace(' ', ''))"
      ],
      "metadata": {
        "id": "_CC6FIJMc40b"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['text'] = tweets['text'].map(spacing)"
      ],
      "metadata": {
        "id": "juZ5qQCCdAHF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['text'] = tweets['text'].map(lambda x : ' '.join(khaiii_analyze(x) if x != '' else ''))"
      ],
      "metadata": {
        "id": "7DFxLaTOdFBd"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['text']"
      ],
      "metadata": {
        "id": "oDVLfGcqdiDY",
        "outputId": "6c492469-8f39-4076-e9a7-e47e5771bd7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       ìœ¤ì„ì—´ ì´ìž¬ëª… ì‚¬ëž‘ ë„ í”¼í•˜ ì‹¬ìƒì • ëŒ€í†µë ¹ ë˜ ë‹¤ìŒ ë™ì„±í˜¼ë²• ì œí™” ì„œ ë‘˜ ê²°í˜¼ì‹ ë¶€...\n",
              "1       ì†ë³´ ëŒ€ ëŒ€í†µë ¹ ì„ ê±° í›„ë³´ í™•ì • ë”ë¶ˆ ë¯¼ì£¼ ë‹¹ì´ìž¬ ëª… êµ­ë¯¼ íž˜ ìœ¤ì„ ì—´ì • ë‹¹ ì‹¬ìƒì •...\n",
              "2       ì •í™•ížˆ í‘œí˜„ í•˜ ì´ìž¬ëª… ì•ˆ ì° ê²ƒ ë‹¹ì—° ê¸°ê¶Œ í•˜ ì´ìž¬ëª… ë„ì™€ì£¼ ê²ƒ íˆ¬í‘œ ë¶ˆì°¸ ë˜í•œ ...\n",
              "3                               ì´ ì‹¬ìƒì • ë¶„ ì–´ë–» ìžŠ ìˆ˜ ìžˆ ë‚˜ ì ˆëŒ€ ëª» ìžŠ\n",
              "4       ì‹¬ìƒì • ë½‘ í‘œ ì‚¬í‘œ ì•„ë‹ˆ ê²ƒ ìš°ë¦¬ ì—¬ì„± ë³´ì´ ì£¼ í•˜ ì¸êµ¬ ì ˆë°˜ ì—¬ì„± ì‚¬í‘œ ì´ í˜‘ë°• ...\n",
              "                              ...                        \n",
              "5316              ìœ¤ì„ì—´ ëŠìž„ì—†ì´ ê¹Œ ì´ìž¬ëª… ëŒ€í•˜ ì¼ì–¸ë°˜êµ¬ ì—† ë‹¹ì‹  ë˜‘ê°™ ì—­ì‚¬ ì£„ì¸ ì´ì´\n",
              "5317    ë‹¤ìŒ ì£¼ í† ìš”ì¼ ì„œì´ˆì§‘íšŒ ì•„ì  ë‹¤ëŒ€ ìž¥ ë™ íŠ¹ê²€ í•©ìˆ˜ë¶€ ì„¤ì¹˜ ì´ìž¬ëª… êµ¬ì† ì†¡ì˜ê¸¸ íƒ„í•µ...\n",
              "5318    ì•„ë‹ˆ ì• ì´ˆ ì´ìž¬ëª… ë¯¼ì£¼ë‹¹ ì•ˆ ë°˜ë¬¸ì§ˆ ì§€ê¸ˆ ì»¤ ì˜¤ ê±° ì‚¬ì‹¤ ì´ ë¬´ìŠ¨ ë¬¸ìž¬ì¸ ìƒê° ì´ìž¬...\n",
              "5319    ì—¬ê¸° ê²½ë¶ ìš¸ì§„ ì£¼ë³€ ë§Ž ì‚¬ëžŒ ê³µí†µ í•˜ ë§ ìœ¤ì„ë ¬ ë„ˆë¬´ ìˆ˜ì¤€ ë–¨ì–´ì§€ ë‹¤ ë°˜ì‘ ë‚´ ê·¸...\n",
              "5320    ê¹€ì¢…í˜ ì „ êµ­ìž¥ ì •ì§„ìƒ ë­ ì´ ê³ ë¦¬ ë˜ ìžˆ ì˜ì‹¬ ë°› ìžˆ ë–¡ í•˜ ë¶€ì‹¤ìž¥ ì•‰ížˆ ê²ƒ ë­ ...\n",
              "Name: text, Length: 5321, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('#4: Tokenization ->', df['text'][0])\n",
        "df['text'] = df['text'].apply(lambda x : [y for y in x.split(' ') if y not in stopwords])\n",
        "print('#5: Stopword ->', df['text'][0])\n",
        "\n",
        "# %% In[4]: Train Model\n",
        "\n",
        "model = tp.LDAModel(k = 20, alpha = 0.5)\n",
        "\n",
        "for i in range(len(df)):\n",
        "    if df['text'][i] != []:\n",
        "        model.add_doc(df['text'][i])\n",
        "\n",
        "for i in range(0, 100, 10):\n",
        "    model.train(10)\n",
        "    print(f'Iteration: {i}, Log-Likelihood: {model.ll_per_word}')\n",
        "\n",
        "# %% In[5]: Show Train Results\n",
        "# for k in range(model.k):\n",
        "#     print(f'Top 10 words of topic #{k}')\n",
        "#     print(model.get_topic_words(k, top_n = 10))\n",
        "\n",
        "# %% In[6]: Save Temp Model\n",
        "model.save('./AI/topic_model.bin')\n"
      ],
      "metadata": {
        "id": "nRwO1R3zbMLf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}